---
title: "154project2"
author: "Jiahua"
date: "4/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
columns <- c("y", "x", "label", "NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")

image1 <- read.table("image_data/image1.txt", header = FALSE, col.names = columns)
image2 <- read.table("image_data/image2.txt", header = FALSE, col.names = columns)
image3 <- read.table("image_data/image3.txt", header = FALSE, col.names = columns)

image1$class <- rep(1, nrow(image1))
image2$class <- rep(2, nrow(image2))
image3$class <- rep(3, nrow(image3))

total_data <- rbind(image1, image2, image3)
```
    
```{r message=FALSE}
library(e1071)
library(ggplot2)
library(corrplot)
library(dplyr)
library(MASS)
library(RColorBrewer)
library(caret)
library(stringr)
```
    
Section 1: Data Collection and Exploration  
1b.  
```{r}
ggplot(data = image1) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (1)") +
  theme_bw()
ggplot(data = image2) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (2)") +
  theme_bw()
ggplot(data = image3) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (3)") +
  theme_bw()
```

1c.
```{r}
# corrplot, label vs all
corrplot(cor(image1[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(image2[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(image3[,3:11]), type="upper",tl.col="black", tl.srt=45)

# Boxplot, label vs NDAI/SD/CORR
gg1_NDAI <- ggplot(data = image1) +
  geom_boxplot(aes(x = factor(label), y = NDAI)) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg1_SD <- ggplot(data = image1) +
  geom_boxplot(aes(x = factor(label), y = SD)) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg1_CORR <- ggplot(data = image1) +
  geom_boxplot(aes(x = factor(label), y = CORR)) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
gg2_NDAI <- ggplot(data = image2) +
  geom_boxplot(aes(x = factor(label), y = NDAI)) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg2_SD <- ggplot(data = image2) +
  geom_boxplot(aes(x = factor(label), y = SD)) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg2_CORR <- ggplot(data = image2) +
  geom_boxplot(aes(x = factor(label), y = CORR)) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
gg3_NDAI <- ggplot(data = image3) +
  geom_boxplot(aes(x = factor(label), y = NDAI)) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg3_SD <- ggplot(data = image3) +
  geom_boxplot(aes(x = factor(label), y = SD)) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg3_CORR <- ggplot(data = image3) +
  geom_boxplot(aes(x = factor(label), y = CORR)) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
```

    
    
Section 2: Preparation   
2a. splitting data by block to address spatial CORRelation.         
```{r}
# specify the number of bin we want on x and y, number of blocks are thus xs * ys
# 30 is a arbitrary decision
set.seed(154)

xs <- 20
ys <- 20
test_size <- 0.2
val_size <- 0.2
'%notin%' <- Negate('%in%') 

total_data2 <- total_data[total_data$label != 0, ]
total_data2$xbins <- cut_interval(total_data2$x, xs, labels = seq(1:xs))
total_data2$ybins <- cut_interval(total_data2$y, ys, labels = seq(1:ys))

total_data2$blocks <- as.numeric(factor(paste0(total_data2$xbins, total_data2$ybins)))

# there may be some blocks without any labeled data
blocks <- length(unique(total_data2$blocks))
test_idx1 <- sample(seq(1:blocks), test_size * blocks)
test_set1 <- total_data2[total_data2$blocks %in% test_idx1, ]
train_set1 <- total_data2[total_data2$blocks %notin% test_idx1, ]

#further splitting trainset into training and validation set.
train_len1 <- length(unique(train_set1$blocks))
val_idx1 <- sample(seq(1:train_len1), val_size * train_len1)
val_set1 <- train_set1[train_set1$blocks %in% val_idx1, ]
train_set1 <- train_set1[train_set1$blocks %notin% val_idx1, ]

hist(total_data2$blocks)
```
        
2a. splitting data using blocks. But this time we split blocks by the equal number of observation in it. Not by the equal area as we have done previously.    
```{r}
set.seed(154)

total_data2$xbins <- cut_number(total_data2$x, xs, labels = seq(1:xs))
total_data2$ybins <- cut_number(total_data2$y, ys, labels = seq(1:ys))

total_data2$blocks <- as.numeric(factor(paste0(total_data2$xbins, total_data2$ybins)))

# there may be some blocks without any labeled data
blocks <- length(unique(total_data2$blocks))
test_idx2 <- sample(seq(1:blocks), test_size * blocks)
test_set2 <- total_data2[total_data2$blocks %in% test_idx2, ]
train_set2 <- total_data2[total_data2$blocks %notin% test_idx2, ]

#further splitting trainset into training and validation set.
train_len2 <- length(unique(train_set2$blocks))
val_idx2 <- sample(seq(1:train_len2), val_size * train_len2)
val_set2 <- train_set2[train_set2$blocks %in% val_idx2, ]
train_set2 <- train_set2[train_set2$blocks %notin% val_idx2, ]

hist(total_data2$blocks)
```


2b.
```{r}
cor(total_data2[,c(3,4,6,11)])
```

2d.
```{r}
loss <- function(label, label_hat){
  ls <- mean(label != label_hat)
  return(ls)
}

CVgeneric <- function(classifier, trainFeature, trainLabel, K = 5, lossfn){
  folds <- createFolds(trainLabel, k = K)
  error_df <- data.frame(error = rep(0, K))
  if(!(str_to_lower(classifier) %in% c("qda", "logistic", "random forest", "KNN"))){
    stop("classifier must be one of qda, logistics, random forest, KNN.")
  }else if(str_to_lower(classifier) == "logistic"){
    for(i in 1:K){
      model <- glm(((trainLabel[-folds[[i]]] + 1)/2) ~ NDAI + CORR + AN, 
                   data = trainFeature[-folds[[i]], ],
                   family = binomial(link = "logit"))
      yhat <- predict(model, trainFeature[folds[[i]], ], type = "response")
      class <- yhat > 0.5
      # error_df[i,1] <- lossfn(as.numeric(class), (trainLabel[folds[[i]]] + 1)/2)
      error_df[i,1] <- mean(as.numeric(class) != (trainLabel[folds[[i]]] + 1)/2)
    }
    return(error_df)
  }
}
CVgeneric("logistic", train1[,c(4,6,11)], train1$label, loss = loss())

```


Section 3: Modeling   
3a.
```{r}
# Split data 1. 
train1 <- rbind(train_set1, val_set1)
folds1 <- createFolds(train1$label, k = 5)

# Logistic
err_logis <- data.frame(error = rep(0, 5))
for(i in 1:5){
  model_logis <- glm(((label + 1)/2) ~ NDAI + CORR + AN, data = train1[-folds1[[i]], ],
                     family = binomial(link = "logit"))
  yhat_logis <- predict(model_logis, train1[folds1[[i]], ], type = "response")
  class_logis <- yhat_logis > 0.5
  err_logis[i,1] <- mean(as.numeric(class_logis) != (train1[folds1[[i]], "label"] + 1)/2)
} 
err_logis

# QDA
err_qda <- data.frame(error = rep(0, 5))
for(i in 1:5){
  model_qda <- qda(label ~ NDAI + CORR + AN, data = train1[-folds1[[i]], ])
  pred_qda <- predict(model_qda, train1[folds1[[i]], ])$class
  # err_qda[i,1] <- mean(pred_qda != train1[folds1[[i]],"label"])
  err_qda[i,1] <- loss(pred_qda, train1[folds1[[i]],"label"])
}
err_qda
```



```{r}
# Split data 1. 
# QDA
model_qda1 <- qda(label ~ NDAI + SD + CORR, data = train_set1) # The Three featrures
pred_qda1 <- predict(model_qda1, val_set1)$class
(err_qda1 <- mean(pred_qda1 != val_set1$label))

model_qda2 <- qda(label ~ NDAI + CORR + AN, data = train_set1) # NDAI, CORR and AN
pred_qda2 <- predict(model_qda2, val_set1)$class
(err_qda2 <- mean(pred_qda2 != val_set1$label))

model_qda3 <- qda(label ~ NDAI + AF + AN, data = train_set1) # NDAI, AF and AN. ------BEST
pred_qda3 <- predict(model_qda3, val_set1)$class
(err_qda3 <- mean(pred_qda3 != val_set1$label))

model_qda4 <- qda(label ~ NDAI + SD + AN, data = train_set1) # NDAI, SD and AN
pred_qda4 <- predict(model_qda4, val_set1)$class
(err_qda4 <- mean(pred_qda4 != val_set1$label))

# Logistic
mode_logis1 <- glm(((label + 1)/2) ~ NDAI + SD + CORR, data = train_set1,
                   family = binomial(link = "logit")) # The Three featrures
yhat_logis1 <- predict(mode_logis1, val_set1, type = "response")
class_logis1 <- yhat_logis1 > 0.5
(err_logis1 <- mean(as.numeric(class_logis1) != (val_set1$label + 1)/2))

mode_logis2 <- glm(((label + 1)/2) ~ NDAI + CORR + AN, data = train_set1,
                   family = binomial(link = "logit"))  # NDAI, CORR and AN
yhat_logis2 <- predict(mode_logis2, val_set1, type = "response")
class_logis2 <- yhat_logis2 > 0.5
(err_logis2 <- mean(as.numeric(class_logis2) != (val_set1$label + 1)/2))

mode_logis3 <- glm(((label + 1)/2) ~ NDAI + AF + AN, data = train_set1,
                   family = binomial(link = "logit")) # NDAI, AF and AN. ------BEST
yhat_logis3 <- predict(mode_logis3, val_set1, type = "response")
class_logis3 <- yhat_logis3 > 0.5
(err_logis3 <- mean(as.numeric(class_logis3) != (val_set1$label + 1)/2))

mode_logis4 <- glm(((label + 1)/2) ~ NDAI + SD + AN, data = train_set1,
                   family = binomial(link = "logit")) # NDAI, SD and AN
yhat_logis4 <- predict(mode_logis4, val_set1, type = "response")
class_logis4 <- yhat_logis4 > 0.5
(err_logis4 <- mean(as.numeric(class_logis4) != (val_set1$label + 1)/2))

# SVM
# model_svm1 = svm(label ~ NDAI + SD + CORR, data = train_set1, kernel = "linear", cost = 10, scale = FALSE)
# print(svmfit)
```




