---
title: "154project2"
author: "Jiahua"
date: "4/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
columns <- c("y", "x", "label", "NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")

image1 <- read.table("image_data/image1.txt", header = FALSE, col.names = columns)
image2 <- read.table("image_data/image2.txt", header = FALSE, col.names = columns)
image3 <- read.table("image_data/image3.txt", header = FALSE, col.names = columns)

image1$class <- rep(1, nrow(image1))
image2$class <- rep(2, nrow(image2))
image3$class <- rep(3, nrow(image3))
```
    
```{r message=FALSE}
library(e1071)
library(ggplot2)
library(corrplot)
library(dplyr)
library(MASS)
library(RColorBrewer)
library(caret)
library(stringr)
library(class)
library(cluster)
library(factoextra)
library(heplots)
```
    
Section 1: Data Collection and Exploration  
1b.  
```{r}
ggplot(data = image1) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (1)") +
  theme_bw()
ggplot(data = image2) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (2)") +
  theme_bw()
ggplot(data = image3) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (3)") +
  theme_bw()
```

1c.
```{r}
# corrplot, label vs all
corrplot(cor(image1[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(image2[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(image3[,3:11]), type="upper",tl.col="black", tl.srt=45)

# Boxplot, label vs NDAI/SD/CORR
gg1_NDAI <- ggplot(data = image1) +
  geom_boxplot(aes(x = factor(label), y = NDAI)) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg1_SD <- ggplot(data = image1) +
  geom_boxplot(aes(x = factor(label), y = SD)) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg1_CORR <- ggplot(data = image1) +
  geom_boxplot(aes(x = factor(label), y = CORR)) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
gg2_NDAI <- ggplot(data = image2) +
  geom_boxplot(aes(x = factor(label), y = NDAI)) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg2_SD <- ggplot(data = image2) +
  geom_boxplot(aes(x = factor(label), y = SD)) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg2_CORR <- ggplot(data = image2) +
  geom_boxplot(aes(x = factor(label), y = CORR)) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
gg3_NDAI <- ggplot(data = image3) +
  geom_boxplot(aes(x = factor(label), y = NDAI)) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg3_SD <- ggplot(data = image3) +
  geom_boxplot(aes(x = factor(label), y = SD)) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg3_CORR <- ggplot(data = image3) +
  geom_boxplot(aes(x = factor(label), y = CORR)) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
```

    
Section 2: Preparation   
2a. defining function to help.        
```{r}
split_data <- function(data, func = cut_interval, xs = 3, ys = 3, size = 0.2, method = 1) {

  #
  #
  #
  #
  #
  
  '%notin%' <- Negate('%in%')
  
  data_no0 <- data[data$label != 0, ]
  if (method == 1) {
    data_no0$xbins <- func(data_no0$x, xs, labels = seq(1:xs))
    data_no0$ybins <- func(data_no0$y, ys, labels = seq(1:ys))
    data_no0$blocks <- as.numeric(factor(paste0(data_no0$xbins, data_no0$ybins)))
  } else {
    helper <- round(data_no0[, c("x", "y")])
    k2 <- kmeans(helper, centers = xs * ys, nstart = 3)
    data_no0$blocks <- k2$cluster
  }
  
  blocks <- length(unique(data_no0$blocks))
  test_idx_temp <- sample(seq(1:blocks), size * blocks)
  test_set_temp <- data_no0[data_no0$blocks %in% test_idx_temp, ]
  train_val_set_temp <- data_no0[data_no0$blocks %notin% test_idx_temp, ]

  #further splitting trainset into training and validation set.
  train_val_len_temp <- length(unique(train_val_set_temp$blocks))
  val_idx_temp <- sample(seq(1:train_val_len_temp), val_size * train_val_len_temp)
  val_set_temp <- train_val_set_temp[train_val_set_temp$blocks %in% val_idx_temp, ]
  train_set_temp <- train_val_set_temp[train_val_set_temp$blocks %notin% val_idx_temp, ]
  
  return(list(train_val_set_temp, train_set_temp, val_set_temp, test_set_temp))
}
```
    
    
splitting data by block of equal area to address spatial CORRelation
```{r}
total_data <- list(image1, image2, image3)
helper <- rbind(image1, image2, image3)
total_data2 <- helper[helper$label != 0, ]

helper1 <- split_data(image1)
helper2 <- split_data(image2)
helper3 <- split_data(image3)

train_val_set1 <- rbind(helper1[[1]], helper2[[1]], helper3[[1]])
train_set1 <- rbind(helper1[[2]], helper2[[2]], helper3[[2]])
val_set1 <- rbind(helper1[[3]], helper2[[3]], helper3[[3]])
test_set1 <- rbind(helper1[[4]], helper2[[4]], helper3[[4]])
```
    
2a. splitting data using blocks. But this time we split blocks by the equal number of observation in it. Not by the equal area as we have done previously. Use cut_number (equal observation) instead of cut_interval (equal length).   
```{r}
set.seed(1542019)
helper1 <- split_data(image1, method = 2)
helper2 <- split_data(image2, method = 2)
helper3 <- split_data(image3, method = 2)

train_val_set2 <- rbind(helper1[[1]], helper2[[1]], helper3[[1]])
train_set2 <- rbind(helper1[[2]], helper2[[2]], helper3[[2]])
val_set2 <- rbind(helper1[[3]], helper2[[3]], helper3[[3]])
test_set2 <- rbind(helper1[[4]], helper2[[4]], helper3[[4]])
```
      
      
```{r}
rbind(train_val_set2, test_set2) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(aes(col = factor(blocks))) + 
  scale_color_manual(values=(brewer.pal(9, "Purples"))) +
  theme_bw()
```
      
      
      
2b. trivial classifier    
```{r}
# accuracy for the trivial classifier that report everything as -1, is just the fraction of -1
# in the original data set
nrow(test_set2[test_set2$label == -1, ])/nrow(test_set2)
```


2c.
```{r}
cor(train_set1[,3:11])
cor(train_set2[,3:11])
corrplot(cor(train_set1[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(train_set2[,3:11]), type="upper",tl.col="black", tl.srt=45)
```



2d.
```{r}
loss <- function(label, label_hat){
  ls <- mean(label != label_hat)
  return(ls)
}

CVgeneric <- function(classifier, trainFeature, trainLabel, K = 5, lossfn){
  
  folds <- createFolds(trainLabel, k = K)
  error_df <- data.frame(error = rep(0, K))
  rownames(error_df) <- 1:K
  
  if(!(str_to_lower(classifier) %in% c("lda", "qda", "logistic", "random forest", "knn"))){
    stop("classifier must be one of qda, logistics, random forest, KNN.")
  }else if(str_to_lower(classifier) == "logistic"){
    for(i in 1:K){
      model <- glm(((trainLabel[-folds[[i]]] + 1)/2) ~ NDAI + CORR + SD, 
                   data = trainFeature[-folds[[i]], ],
                   family = binomial(link = "logit"))
      yhat <- predict(model, trainFeature[folds[[i]], ], type = "response")
      class <- yhat > 0.5
      error_df[i,1] <- lossfn(as.numeric(class), ((trainLabel[folds[[i]]] + 1)/2))
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "qda"){
    for(i in 1:K){
      model <- qda(trainLabel[-folds[[i]]] ~ NDAI + CORR + SD, data = trainFeature[-folds[[i]], ])
      pred_qda <- predict(model, trainFeature[folds[[i]], ])$class
      error_df[i,1] <- lossfn(pred_qda, trainLabel[folds[[i]]])
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "lda"){
    for(i in 1:K){
      model <- lda(trainLabel[-folds[[i]]] ~ NDAI + CORR + SD, data = trainFeature[-folds[[i]], ])
      pred_lda <- predict(model, trainFeature[folds[[i]], ])$class
      error_df[i,1] <- lossfn(pred_lda, trainLabel[folds[[i]]])
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "knn"){
    for(i in 1:K){
      model <- knn(train = trainFeature[-folds[[i]], ], test = trainFeature[folds[[i]], ],
             cl = trainLabel[-folds[[i]]], k = 5)
      error_df[i,1] <- lossfn(model, trainLabel[folds[[i]]])
    }
    return(error_df)
    
  }
}

# CVgeneric("logistic", train_set1[,4:6], train_set1$label, lossfn = loss)
# CVgeneric("qda", train_set1[,4:6], train_set1$label, lossfn = loss)
# CVgeneric("KNN", train_set1[,4:6], train_set1$label, lossfn = loss)

```


Section 3: Modeling   
3a.
```{r}
# Split data 1. 

# Train

# Logistic
err_logis1 <- CVgeneric("logistic", train_val_set1[,4:6], train_val_set1$label, lossfn = loss)
err_logis1

# LDA
err_lda1 <- CVgeneric("LDA", train_val_set1[,4:6], train_val_set1$label, lossfn = loss)
err_lda1

# QDA
err_qda1 <- CVgeneric("QDA", train_val_set1[,4:6], train_val_set1$label, lossfn = loss)
err_qda1

# KNN
err_knn1 <- CVgeneric("KNN", train_val_set1[,4:6], train_val_set1$label, lossfn = loss)
err_knn1

# Summary
error_split1 <- data.frame(logistic = err_logis1, lda = err_lda1, qda = err_qda1, KNN = err_knn1)
colnames(error_split1) <- c("logistic", "LDA", "QDA", "KNN")

```

```{r}
# # Assumption
# 
# # Plot
# ggplot(data = train_val_set1) +
#   geom_point(aes(x = NDAI, y = CORR, shape = factor(label), color = factor(label)), alpha = 0.5) + 
#   scale_color_manual(values=c("#66c2a5", "#fc8d62"))
# ggplot(data = train_val_set1) +
#   geom_point(aes(x = NDAI, y = AN, shape = factor(label), color = factor(label)), alpha = 0.5) + 
#   scale_color_manual(values=c("#66c2a5", "#fc8d62"))
# ggplot(data = train_val_set1) +
#   geom_point(aes(x = CORR, y = AN, shape = factor(label), color = factor(label)), alpha = 0.5) + 
#   scale_color_manual(values=c("#66c2a5", "#fc8d62"))

# LDA & QDA, equal covariance
covEllipses(train_set1[,4:6], factor(train_set1$label), 
            fill = TRUE, pooled = FALSE,
            col = c("blue", "red"), 
            variables = 1:3,
            fill.alpha = 0.05)
boxM(train_set1[, 4:6], factor(train_set1$label)) # reject equal covafiance

covEllipses(train_set2[,4:6], factor(train_set2$label), 
            fill = TRUE, pooled = FALSE,
            col = c("blue", "red"), 
            variables = 1:3,
            fill.alpha = 0.05)
boxM(train_set2[, 4:6], factor(train_set2$label))
# LDA & QDA, Gaussian
par(mfrow = c(1,3))
for(i in 4:6){
  qqnorm(train_set1[which(train_set1$label == 1),i], ylab = colnames(train_set1)[i])
  qqline(train_set1[which(train_set1$label == 1),i], col = 2)
}
for(i in 4:6){
  qqnorm(train_set1[which(train_set1$label == -1),i], ylab = colnames(train_set2)[i])
  qqline(train_set1[which(train_set1$label == -1),i], col = 2)
}

```

```{r}
# Split data 2. 

# Logistic
err_logis2 <- CVgeneric("logistic", train_val_set2[,4:6], train_val_set2$label, lossfn = loss)
err_logis2

# LDA
err_lda2 <- CVgeneric("LDA", train_val_set2[,4:6], train_val_set2$label, lossfn = loss)
err_lda2

# QDA
err_qda2 <- CVgeneric("QDA", train_val_set2[,4:6], train_val_set2$label, lossfn = loss)
err_qda2

# KNN
err_knn2 <- CVgeneric("KNN", train_val_set2[,4:6], train_val_set2$label, lossfn = loss)
err_knn2

# Summary
error_split2 <- data.frame(logistic = err_logis2, lda = err_lda2, qda = err_qda2, KNN = err_knn2)
colnames(error_split2) <- c("logistic", "LDA", "QDA", "KNN")

```

```{r}
agreement1 <- rbind(1 - error_split1, colMeans(1 - error_split1))
rownames(agreement1)[nrow(agreement1)] <- "average"

agreement2 <- rbind(1 - error_split2, colMeans(1 - error_split2))
rownames(agreement2)[nrow(agreement2)] <- "average"

agreement1
agreement2
```





