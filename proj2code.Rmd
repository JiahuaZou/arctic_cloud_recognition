---
title: "154project2"
author: "Jiahua"
date: "4/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
columns <- c("y", "x", "label", "NDAI", "SD", "CORR", "DF", "CF", "BF", "AF", "AN")
image1 <- read.table("image_data/image1.txt", header = FALSE, col.names = columns)
image2 <- read.table("image_data/image2.txt", header = FALSE, col.names = columns)
image3 <- read.table("image_data/image3.txt", header = FALSE, col.names = columns)
image1$class <- rep(1, nrow(image1))
image2$class <- rep(2, nrow(image2))
image3$class <- rep(3, nrow(image3))
```
    
```{r message=FALSE}
library(e1071)
library(ggplot2)
library(corrplot)
library(dplyr)
library(MASS)
library(RColorBrewer)
library(caret)
library(stringr)
library(class)
library(cluster)
library(factoextra)
library(heplots)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(gridExtra)
```
  
  
define an operator called notin, and a loss function tha calculate accuracy of classifier
```{r}
'%notin%' <- Negate('%in%')
loss <- function(label, label_hat){
  ls <- mean(label != label_hat)
  return(ls)
}
```
  
    
Section 1: Data Collection and Exploration  
1b. cloud/clear plot
```{r}
# percent
perc1 <- c(table(image1$label)/nrow(image1), nrow(image1))
perc2 <- c(table(image2$label)/nrow(image2), nrow(image2))
perc3 <- c(table(image3$label)/nrow(image3), nrow(image3))
perc_all <- c((table(image1$label)+table(image2$label)+table(image3$label))/(nrow(image1)+nrow(image2)+nrow(image3)),
             (nrow(image1)+nrow(image2)+nrow(image3)))
perc <- rbind(perc1, perc2, perc3, perc_all)
colnames(perc) <- c("clear", "unlabeled", "cloud", "obs")
rownames(perc) <- c("image1", "image2", "image3", "overall")

# distribution plot
ggplot(data = image1) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (1)") +
  theme_bw()
ggplot(data = image2) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (2)") +
  theme_bw()
ggplot(data = image3) +
  geom_point(aes(x = x, y = y, color = factor(label))) +
  scale_color_manual(values=(brewer.pal(3, "Purples"))[3:1]) +
  scale_x_continuous(breaks = seq(0, 400, 50)) +
  scale_y_continuous(breaks = seq(65, 370, 50)) +
  xlab("x coordinate") +
  ylab("y coordinate") +
  ggtitle("distribution of cloud and non-cloud (3)") +
  theme_bw()
```

1c. cor-plot and boxplot
```{r}
# corrplot, label vs all
corrplot(cor(image1[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(image2[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(image3[,3:11]), type="upper",tl.col="black", tl.srt=45)
# Boxplot, label vs NDAI/SD/CORR
# ----delete 0
image1_0 <- image1[which(image1$label != 0), ]
image2_0 <- image2[which(image2$label != 0), ]
image3_0 <- image3[which(image3$label != 0), ]
# ----
gg1_NDAI <- ggplot() +
  geom_boxplot(aes(x = factor(image1_0$label), y = image1_0$NDAI), outlier.size = 0.5) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg1_SD <- ggplot() +
  geom_boxplot(aes(x = factor(image1_0$label), y = image1_0$SD), outlier.size = 0.5) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg1_CORR <- ggplot() +
  geom_boxplot(aes(x = factor(image1_0$label), y = image1_0$CORR), outlier.size = 0.5) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
gg2_NDAI <- ggplot() +
  geom_boxplot(aes(x = factor(image2_0$label), y = image2_0$NDAI), outlier.size = 0.5) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg2_SD <- ggplot() +
  geom_boxplot(aes(x = factor(image2_0$label), y = image2_0$SD), outlier.size = 0.5) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg2_CORR <- ggplot() +
  geom_boxplot(aes(x = factor(image2_0$label), y = image2_0$CORR), outlier.size = 0.5) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()
gg3_NDAI <- ggplot() +
  geom_boxplot(aes(x = factor(image3_0$label), y = image3_0$NDAI), outlier.size = 0.5) +
  xlab("Label") +
  ylab("NDAI") +
  ggtitle("label vs NDAI") +
  theme_bw()
gg3_SD <- ggplot() +
  geom_boxplot(aes(x = factor(image3_0$label), y = image3_0$SD), outlier.size = 0.5) +
  xlab("Label") +
  ylab("SD") +
  ggtitle("label vs SD") +
  theme_bw()
gg3_CORR <- ggplot() +
  geom_boxplot(aes(x = factor(image3_0$label), y = image3_0$CORR), outlier.size = 0.5) +
  xlab("Label") +
  ylab("CORR") +
  ggtitle("label vs CORR") +
  theme_bw()

jpeg("feature_boxplot", quality = 100)
grid.arrange(gg1_NDAI, gg1_SD, gg1_CORR, 
             gg2_NDAI, gg2_SD, gg2_CORR, 
             gg3_NDAI, gg3_SD, gg3_CORR, 
             nrow = 3)
dev.off()
```

    
Section 2: Preparation   
2a. defining function to help.        
```{r}
split_data <- function(data, func = cut_interval, xs = 3, ys = 3, test_size = 0.2, 
                       val_size = 0.2, method = 1) {
#' @title split data function
#' @description split data by assigning a block number to it according to its geographical location
#' @param func the function used to split the x values and y values, can take in cut_interval, which cut the axis to equal length
#' @param xs number of interval in x axis, default 3
#' @param ys number of interval in y axis, default 3, the number of blocks per image is calculated by xs * ys, so by default each image has 9 blocks
#' @param test_size the size of test set
#' @param val_size the size of validation set
#' @param method if it is 1, then split the image in blocks of equal area, else, use 9-means clustering to split the image to 9 clusters
#' @return a list of 4 dataset, the first is the combination of training and validation set, the second is training set, the third is the validation set, and the fourth is the test set.
  
  data_no0 <- data[data$label != 0, ]
  if (method == 1) {
    data_no0$xbins <- func(data_no0$x, xs, labels = seq(1:xs))
    data_no0$ybins <- func(data_no0$y, ys, labels = seq(1:ys))
    data_no0$blocks <- as.numeric(factor(paste0(data_no0$xbins, data_no0$ybins)))
  } else {
    helper <- round(data_no0[, c("x", "y")])
    k2 <- kmeans(helper, centers = xs * ys, nstart = 3)
    data_no0$blocks <- k2$cluster
  }
  
  blocks <- length(unique(data_no0$blocks))
  test_idx_temp <- sample(seq(1:blocks), test_size * blocks)
  test_set_temp <- data_no0[data_no0$blocks %in% test_idx_temp, ]
  train_val_set_temp <- data_no0[data_no0$blocks %notin% test_idx_temp, ]
  #further splitting trainset into training and validation set.
  train_val_len_temp <- length(unique(train_val_set_temp$blocks))
  val_idx_temp <- sample(seq(1:train_val_len_temp), val_size * train_val_len_temp)
  val_set_temp <- train_val_set_temp[train_val_set_temp$blocks %in% val_idx_temp, ]
  train_set_temp <- train_val_set_temp[train_val_set_temp$blocks %notin% val_idx_temp, ]
  
  return(list(train_val_set_temp, train_set_temp, val_set_temp, test_set_temp))
}
```
    
cheking if the number of observtions of blocks looks like a uniform distribution.  
```{r}
set.seed(1542019)
helper1 <- as.data.frame(table(split_data(rbind(image1, image2, image3))[[1]]$blocks))
helper2 <- as.data.frame(table(split_data(rbind(image1, image2, image3), 
                                          xs = 2, ys = 2)[[1]]$blocks))
helper3 <- as.data.frame(table(split_data(rbind(image1, image2, image3), 
                                          xs = 4, ys = 4)[[1]]$blocks))
colnames(helper1) <- c("block", "frequency")
colnames(helper2) <- c("block", "frequency")
colnames(helper3) <- c("block", "frequency")
helper1 %>% ggplot(aes(x = block, y = frequency)) + geom_bar(stat="identity") + 
  ggtitle("9 blocks per image")
helper2 %>% ggplot(aes(x = block, y = frequency)) + geom_bar(stat="identity") + 
  ggtitle("4 blocks per image")
helper3 %>% ggplot(aes(x = block, y = frequency)) + geom_bar(stat="identity") + 
  ggtitle("16 blocks per image")
```
    
2a.  splitting data by block of equal area to address spatial CORRelation
```{r}
set.seed(154)
helper11 <- split_data(image1)
helper12 <- split_data(image2)
helper13 <- split_data(image3)
train_val_set1 <- rbind(helper11[[1]], helper12[[1]], helper13[[1]])
train_set1 <- rbind(helper11[[2]], helper12[[2]], helper13[[2]])
val_set1 <- rbind(helper11[[3]], helper12[[3]], helper13[[3]])
test_set1 <- rbind(helper11[[4]], helper12[[4]], helper13[[4]])
```
    
2a continued. splitting data using blocks. But this time we split blocks by the equal number of observation in it. Not by the equal area as we have done previously. Use cut_number (equal observation) instead of cut_interval (equal length).   
```{r}
set.seed(1542019)
helper1 <- split_data(image1, method = 2)
helper2 <- split_data(image2, method = 2)
helper3 <- split_data(image3, method = 2)
train_val_set2 <- rbind(helper1[[1]], helper2[[1]], helper3[[1]])
train_set2 <- rbind(helper1[[2]], helper2[[2]], helper3[[2]])
val_set2 <- rbind(helper1[[3]], helper2[[3]], helper3[[3]])
test_set2 <- rbind(helper1[[4]], helper2[[4]], helper3[[4]])
```
      
      
```{r}
set.seed(1542019)
helper <- split_data(image1)
helper1 <- rbind(helper[[1]], helper[[4]])
helper <- split_data(image1, method = 2)
helper2 <- rbind(helper[[1]], helper[[4]])
helper1 %>% ggplot(aes(x = x, y = y)) + 
  geom_raster(aes(fill = factor(blocks))) +
  scale_color_manual(values=(brewer.pal(9, "Purples"))) + theme_bw() + 
  ggtitle("Method 1: Splitting by square block of equal area for image 1")
helper2 %>% ggplot(aes(x = x, y = y)) + 
  geom_raster(aes(fill = factor(blocks))) +
  scale_color_manual(values=(brewer.pal(9, "Purples"))) + theme_bw() +
  ggtitle("Method 2: Splitting by K Means Clustering with k = 9 for image 1")
```
      
      
      
2b. trivial classifier    
```{r}
# accuracy for the trivial classifier that report everything as -1, is just the fraction of -1
# in the original data set
helper <- table(rbind(train_val_set1, test_set1)$label)
helper[1]/sum(helper)
```


2c. pick three features
```{r}
cor(train_set1[,3:11])
cor(train_set2[,3:11])
corrplot(cor(train_set1[,3:11]), type="upper",tl.col="black", tl.srt=45)
corrplot(cor(train_set2[,3:11]), type="upper",tl.col="black", tl.srt=45)
```


2d. defining loss function and CV generic
```{r}
loss <- function(label, label_hat){
  # loss function to compute error rate.
  ls <- mean(label != label_hat)
  return(ls)
}

#' @title CV generic function
#' @description aggregate classification methods including GDA, Logistic, random forest, knn and classification tree
#' @param classifier the classification method
#' @param trainFeature the features in train data set
#' @param trainLabel the label in train data set
#' @param K the parameter for k-fold CV
#' @param lossfn the loss function defined to compute error/accuracy
#' @return the combination of k successes can occur in n trials
CVgeneric <- function(classifier, trainFeature, trainLabel, K = 8, lossfn){

  set.seed(154)
  num_blocks <- unique(trainFeature$blocks)
  folds <- createFolds(num_blocks, K)
  error_df <- matrix(0, nrow = K, ncol = 1)
  rownames(error_df) <- 1:K
  
  if(!(str_to_lower(classifier) %in% c("lda", "qda", "logistic", "randomforest", "knn", "tree"))){
    stop("classifier must be one of lda, qda, logistics, randomforest, KNN or tree.")
  }else if(str_to_lower(classifier) == "logistic"){
    for(i in 1:K){
      model <- glm(((trainLabel$label[trainLabel$blocks %notin% folds[[i]]] + 1)/2) ~ 
                     NDAI + CORR + SD, 
                   data = trainFeature[trainFeature$blocks %notin% folds[[i]], 1:3],
                   family = binomial(link = "logit"))
      yhat <- predict(model, trainFeature[trainFeature$blocks %in% folds[[i]], 1:3], type = "response")
      class <- yhat > 0.5
      error_df[i,1] <- lossfn(as.numeric(class),
                              ((trainLabel$label[trainLabel$blocks %in% folds[[i]]] + 1)/2))
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "qda"){
    for(i in 1:K){
      model <- qda(trainLabel$label[trainLabel$blocks %notin% folds[[i]]] ~ NDAI + CORR + SD,
                   data = trainFeature[trainFeature$blocks %notin% folds[[i]], 1:3])
      pred_qda <- predict(model, trainFeature[trainFeature$blocks %in% folds[[i]], 1:3])$class
      error_df[i,1] <- lossfn(pred_qda, trainLabel$label[trainLabel$blocks %in% folds[[i]]])
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "lda"){
    for(i in 1:K){
      model <- lda(trainLabel$label[trainLabel$blocks %notin% folds[[i]]] ~ NDAI + CORR + SD, data = trainFeature[trainFeature$blocks %notin% folds[[i]], 1:3])
      pred_lda <- predict(model, trainFeature[trainFeature$blocks %in% folds[[i]], 1:3])$class
      error_df[i,1] <- lossfn(pred_lda, trainLabel$label[trainLabel$blocks %in% folds[[i]]])
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "knn"){
    for(i in 1:K){
      model <- knn(train = trainFeature[trainFeature$blocks %notin% folds[[i]], 1:3], 
                   test = trainFeature[trainFeature$blocks %in% folds[[i]], 1:3],
                   cl = trainLabel$label[trainLabel$blocks %notin% folds[[i]]],
                   k = 50, prob = TRUE)
      error_df[i,1] <- lossfn(model, trainLabel$label[trainLabel$blocks %in% folds[[i]]])
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "tree") {
    for (i in 1:K) {
      model <- rpart(factor(trainLabel$label[trainLabel$blocks %notin% folds[[i]]]) ~ NDAI + CORR + SD, 
                     data = trainFeature[trainFeature$blocks %notin% folds[[i]], 1:3])
      pred_tree <- predict(model, trainFeature[trainFeature$blocks %in% folds[[i]], 1:3], type = "class")
      yhat <- as.numeric(as.character(pred_tree))
      error_df[i,1] <- lossfn(yhat, trainLabel$label[trainLabel$blocks %in% folds[[i]]])
    }
    return(error_df)
    
  }else if(str_to_lower(classifier) == "randomforest") {
    for (i in 1:K) {
      model <- randomForest(factor(trainLabel$label[trainLabel$blocks %notin% folds[[i]]]) ~NDAI + CORR + SD,data = trainFeature[trainFeature$blocks %notin% folds[[i]], 1:3])
      pred_rf <- predict(model, trainFeature[trainFeature$blocks %in% folds[[i]], 1:3],
type = "class")
      yhat <- as.numeric(as.character(pred_rf))
      error_df[i,1] <- lossfn(yhat, trainLabel$label[trainLabel$blocks %in% folds[[i]]])
    }
    return(error_df)
  }
}
```
    
use 9 fold cross validation on knn to determine the best k
```{r}
set.seed(1542019)
num_blocks <- unique(train_val_set1$blocks)
folds <- createFolds(num_blocks, length(num_blocks)/2)
knn_error_df <- matrix(0, nrow = length(folds), ncol = 5)
for(i in 1:length(folds)){
  j <- 1
  for (k in c(20, 30, 40, 50, 60)) {
    model <- knn(train = train_val_set1[train_val_set1$blocks %notin% folds[[i]], 4:6], 
                 test = train_val_set1[train_val_set1$blocks %in% folds[[i]], 4:6],
                 cl = train_val_set1$label[train_val_set1$blocks %notin% folds[[i]]], k = k)
    knn_error_df[i,j] <- loss(model, 
                              train_val_set1$label[train_val_set1$blocks %in% folds[[i]]])
    j <- j + 1
  }
}
knn_error_df <- rbind(knn_error_df, colMeans(knn_error_df))
colnames(knn_error_df) <- c(20, 30, 40, 50, 60)
rownames(knn_error_df) <- c("cv1", "cv2", "cv3", "cv4", "average")
knn_error_df
```
    
        
plotting to see the accuracy of knn with different k
```{r}
plot(c(20, 30, 40, 50, 60), 1 - knn_error_df[,5], xlab = "value of K", ylab = "cross validation accuracy")
lines(c(20, 30, 40, 50, 60), 1 - knn_error_df[,5])
```
    
    
Section 3: Modeling   
3a.
using data 1
```{r}
# Split data 1. 
# Train
# Logistic
err_logis1 <- CVgeneric("logistic", train_val_set1[,c(4:6, 15)], 
                        train_val_set1[, c(3, 15)], lossfn = loss)
err_logis1
# LDA
err_lda1 <- CVgeneric("LDA", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = loss)
err_lda1
# QDA
err_qda1 <- CVgeneric("QDA", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = loss)
err_qda1
# KNN
err_knn1 <- CVgeneric("KNN", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = loss)
err_knn1
# classification tree
err_tree1 <- CVgeneric("tree", train_val_set1[,c(4:6, 15)], 
                       train_val_set1[,c(3,15)], lossfn = loss)
err_tree1
# random forest
err_rf1 <- CVgeneric("randomforest", train_val_set1[,c(4:6, 15)], 
                     train_val_set1[,c(3,15)], lossfn = loss)
err_rf1
# Summary
error_split1 <- data.frame(logistic = err_logis1, lda = err_lda1, qda = err_qda1, KNN = err_knn1, Tree = err_tree1, rf = err_rf1)
colnames(error_split1) <- c("logistic", "LDA", "QDA", "KNN", "Tree", "Random Forest")
error_split1 <- rbind(error_split1, colMeans(error_split1))
```

distribution
```{r}
# Assumption
ggplot(data = train1) +
  geom_point(aes(x = NDAI, y = CORR, shape = factor(label), color = factor(label))) + 
  scale_color_manual(values=c("#66c2a5", "#fc8d62"))
ggplot(data = train1) +
  geom_point(aes(x = NDAI, y = SD, shape = factor(label), color = factor(label))) + 
  scale_color_manual(values=c("#66c2a5", "#fc8d62"))
ggplot(data = train1) +
  geom_point(aes(x = CORR, y = SD, shape = factor(label), color = factor(label))) + 
  scale_color_manual(values=c("#66c2a5", "#fc8d62"))
```

data 2
```{r}
# Split data 2. 
# logistic
err_logis2 <- CVgeneric("logistic", train_val_set2[, c(4:6,13)], 
                        train_val_set2[, c(3, 13)], lossfn = loss)
err_logis2
# LDA
err_lda2 <- CVgeneric("LDA", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = loss)
err_lda2
# QDA
err_qda2 <- CVgeneric("QDA", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = loss)
err_qda2
# KNN
err_knn2 <- CVgeneric("KNN", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = loss)
err_knn2
# classification tree
err_tree2 <- CVgeneric("tree", train_val_set2[,c(4:6, 13)], 
                       train_val_set2[,c(3,13)], lossfn = loss)
err_tree2
# random forest
err_rf2 <- CVgeneric("randomforest", train_val_set2[,c(4:6, 13)], 
                     train_val_set2[,c(3,13)], lossfn = loss)
err_rf2
# Summary
error_split2 <- data.frame(logistic = err_logis2, lda = err_lda2, qda = err_qda2, KNN = err_knn2, Tree = err_tree2, rf = err_rf2)
colnames(error_split2) <- c("logistic", "LDA", "QDA", "KNN", "Tree", "Random Forest")
```
    
    
Train the model on full train_val_set and test it on testset
```{r}
logistic_model1 <- glm(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set1,
                      family = binomial(link = "logit"))
yhat_logis1 <- predict(logistic_model1, test_set1, type = "response")
logistic_test_accuracy1 <- 1 - loss(as.numeric(yhat_logis1 > 0.5), (as.numeric(test_set1$label)+1)/2)
logistic_test_accuracy1

lda_model1 <- lda(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set1)
yhat_lda1 <- predict(lda_model1, test_set1)$class
lda_test_accuracy1 <- 1 - loss(yhat_lda1, test_set1$label)
lda_test_accuracy1

qda_model1 <- qda(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set1)
yhat_qda1 <- predict(qda_model1, test_set1)$class
qda_test_accuracy1 <- 1 - loss(yhat_qda1, test_set1$label)
qda_test_accuracy1

knn_model1 <- knn(train_val_set1[,4:6], test_set1[,4:6], as.factor(train_val_set1[,3]), k = 50, prob = TRUE)
knn_test_accuracy1 <- 1 - loss(knn_model1, test_set1[,3])
knn_test_accuracy1

tree_model1 <- rpart(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set1)
yhat_tree1 <- as.numeric(as.character(predict(tree_model1, test_set1, type = "class")))
tree_test_accuracy1 <- 1 - loss(as.numeric(yhat_tree1), test_set1$label)
tree_test_accuracy1

randomforest_model1 <- randomForest(as.factor(label) ~ NDAI + CORR + SD, 
                                    data = train_val_set1)
yhat_rf1 <- as.numeric(as.character(predict(randomforest_model1, test_set1, type = "class")))
rf_test_accuracy1 <- 1 - loss(as.numeric(yhat_rf1), test_set1$label)
rf_test_accuracy1
```
  
On the second data split
```{r}
logistic_model2 <- glm(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set2,
                      family = binomial(link = "logit"))
yhat_logis2 <- predict(logistic_model2, test_set2, type = "response")
logistic_test_accuracy2 <- 1 - loss(as.numeric(yhat_logis2 > 0.5), (as.numeric(test_set2$label)+1)/2)
logistic_test_accuracy2

lda_model2 <- lda(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set2)
yhat_lda2 <- predict(lda_model2, test_set2)$class
lda_test_accuracy2 <- 1 - loss(yhat_lda2, test_set2$label)
lda_test_accuracy2

qda_model2 <- qda(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set2)
yhat_qda2 <- predict(qda_model2, test_set2)$class
qda_test_accuracy2 <- 1 - loss(yhat_qda2, test_set2$label)
qda_test_accuracy2

knn_model2 <- knn(train_val_set2[,4:6], test_set2[,4:6], as.factor(train_val_set2[,3]), k = 40, prob = TRUE)
knn_test_accuracy2 <- 1 - loss(knn_model2, test_set2[,3])
knn_test_accuracy2

tree_model2 <- rpart(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set2)
yhat_tree2 <- as.numeric(as.character(predict(tree_model2, test_set2, type = "class")))
tree_test_accuracy2 <- 1 - loss(as.numeric(yhat_tree2), test_set2$label)
tree_test_accuracy2

randomforest_model2 <- randomForest(as.factor(label) ~ NDAI + CORR + SD, data = train_val_set2)
yhat_rf2 <- as.numeric(as.character(predict(randomforest_model2, test_set2, type = "class")))
rf_test_accuracy2 <- 1 - loss(as.numeric(yhat_rf2), test_set2$label)
rf_test_accuracy2


```
    
LDA QDA assumption
```{r}
# <<<<<<< HEAD
# Assumption for data 2
# LDA & QDA, equal covariance
covEllipses(train_set1[,4:6], factor(train_set1$label), 
            fill = TRUE, pooled = FALSE,
            col = c("blue", "red"), 
            variables = 1:3,
            fill.alpha = 0.05)
boxM(train_set1[, 4:6], factor(train_set1$label)) # reject equal covafiance
covEllipses(train_set2[,4:6], factor(train_set2$label), 
            fill = TRUE, pooled = FALSE,
            col = c("blue", "red"), 
            variables = 1:3,
            fill.alpha = 0.05)
boxM(train_set2[, 4:6], factor(train_set2$label))
# LDA & QDA, Gaussian
par(mfrow = c(1,3))
for(i in 4:6){
  qqnorm(train_set1[which(train_set1$label == 1),i], ylab = colnames(train_set1)[i])
  qqline(train_set1[which(train_set1$label == 1),i], col = 2)
}
for(i in 4:6){
  qqnorm(train_set1[which(train_set1$label == -1),i], ylab = colnames(train_set1)[i])
  qqline(train_set1[which(train_set1$label == -1),i], col = 2)
}
for(i in 4:6){
  qqnorm(train_set2[which(train_set2$label == 1),i], ylab = colnames(train_set2)[i])
  qqline(train_set2[which(train_set2$label == 1),i], col = 2)
}
for(i in 4:6){
  qqnorm(train_set2[which(train_set2$label == -1),i], ylab = colnames(train_set2)[i])
  qqline(train_set2[which(train_set2$label == -1),i], col = 2)
}
par(mfrow = c(1,3))
```


```{r}
agreement1 <- rbind(1 - error_split1, colMeans(1 - error_split1))
agreement1 <- 1 - error_split1
rownames(agreement1)[nrow(agreement1)] <- "average"
agreement2 <- 1 - error_split2
rownames(agreement2)[nrow(agreement2)] <- "average"
agreement1
agreement2
```
    

```{r}
test_accuracy_all <- as.data.frame(rbind(c(logistic_test_accuracy1, lda_test_accuracy1, qda_test_accuracy1,
                                           knn_test_accuracy1, tree_test_accuracy1, rf_test_accuracy1), 
                                         c(logistic_test_accuracy2, lda_test_accuracy2, qda_test_accuracy2,
                                           knn_test_accuracy2, tree_test_accuracy2, rf_test_accuracy2)))
rownames(test_accuracy_all) <- c("test set 1", "test set 2")
colnames(test_accuracy_all) <- c("logistic", "LDA", "QDA", "KNN", "Tree", "Random Forest")
test_accuracy_all
```


3b. ROC
ROC on data 1
```{r}
# on data 1
prob_logis1 <- predict(logistic_model1, test_set1, type = "response")

prob_lda1 <- predict(lda_model1, test_set1)$posterior[,2]

prob_qda1 <- predict(qda_model1, test_set1)$posterior[,2]

prob_knn1 <- attributes(knn_model1)$prob

prob_tree1 <- as.numeric(as.character(predict(tree_model1, test_set1, type = "prob")))[1:30227]

prob_rf1 <- as.numeric(as.character(predict(randomforest_model1, test_set1, type = "prob")))[1:30227]

par(mfrow = c(2,3), pty = "s")
roc_logis1 <- roc(test_set1$label, prob_logis1, auc = TRUE)
index_logis1 <- which.min(abs(roc_logis1$sensitivities - roc_logis1$specificities)[-1])
plot(roc_logis1, print.thres = roc_logis1$thresholds[index_logis1], col = "red", 
     asp = NA, legacy.axes = TRUE, main = "ROC of Logistics")

roc_lda1 <- roc(test_set1$label, prob_lda1, auc = TRUE)
index_lda1 <- which.min(abs(roc_lda1$sensitivities - roc_lda1$specificities)[-1])
plot(roc_lda1, print.thres = roc_lda1$thresholds[index_lda1], col = "red", 
     asp = NA, legacy.axes = TRUE, main = "ROC of LDA")

roc_qda1 <- roc(test_set1$label, prob_qda1, auc = TRUE)
index_qda1 <- which.min(abs(roc_qda1$sensitivities - roc_qda1$specificities)[-1])
plot(roc_qda1, print.thres = roc_qda1$thresholds[index_qda1], col = "red", 
     asp = NA, legacy.axes = TRUE, main = "ROC of QDA")

roc_knn1 <- roc(test_set1$label, prob_knn1, auc = TRUE)
plot(roc_knn1, print.thres = roc_knn1$thresholds[2], col = "red",
     asp = NA, legacy.axes = TRUE, main = "ROC of KNN")

roc_tree1 <- roc(test_set1$label, prob_tree1, auc = TRUE)
plot(roc_tree1, print.thres = roc_tree1$thresholds[2], col = "red",
     asp = NA, legacy.axes = TRUE, main = "ROC of Tree")


roc_rf1 <- roc(test_set1$label, prob_rf1, auc = TRUE)
index_rf1 <- which.min(abs(roc_rf1$sensitivities - roc_rf1$specificities)[-1])
plot(roc_rf1, print.thres = roc_rf1$thresholds[index_rf1], col = "red",
     asp = NA, legacy.axes = TRUE, main = "ROC of RF")

roc1 <- c(roc_logis1$auc, roc_lda1$auc, roc_qda1$auc, roc_knn1$auc, roc_tree1$auc, roc_rf1$auc)
names(roc1) <- c("Logistics", "LDA", "QDA", "KNN", "Tree", "Random Forest")
roc1
```

ROC on data 2
```{r}
# on data 2
prob_logis2 <- predict(logistic_model2, test_set2, type = "response")

prob_lda2 <- predict(lda_model2, test_set2)$posterior[,2]

prob_qda2 <- predict(qda_model2, test_set2)$posterior[,2]

prob_knn2 <- attributes(knn_model2)$prob

prob_tree2 <- as.numeric(as.character(predict(tree_model2, test_set2, type = "prob")))[1:20983]

prob_rf2 <- as.numeric(as.character(predict(randomforest_model2, test_set2, type = "prob")))[1:20983]

par(mfrow = c(2,3), pty = "s")
roc_logis2 <- roc(test_set2$label, prob_logis2, auc = TRUE)
index_logis2 <- which.min(abs(roc_logis2$sensitivities - roc_logis2$specificities)[-1])
plot(roc_logis2, print.thres = roc_logis2$thresholds[index_logis2], col = "red", 
     asp = NA, legacy.axes = TRUE, main = "ROC of Logistics")

roc_lda2 <- roc(test_set2$label, prob_lda2, auc = TRUE)
index_lda2 <- which.min(abs(roc_lda2$sensitivities - roc_lda2$specificities)[-1])
plot(roc_lda2, print.thres = roc_lda2$thresholds[index_lda2], col = "red", 
     asp = NA, legacy.axes = TRUE, main = "ROC of LDA")

roc_qda2 <- roc(test_set2$label, prob_qda2, auc = TRUE)
index_qda2 <- which.min(abs(roc_qda2$sensitivities - roc_qda2$specificities)[-1])
plot(roc_qda2, print.thres = roc_qda2$thresholds[index_qda2], col = "red", 
     asp = NA, legacy.axes = TRUE, main = "ROC of QDA")

roc_knn2 <- roc(test_set2$label, prob_knn2, auc = TRUE)
plot(roc_knn2, print.thres = roc_knn2$thresholds[2], col = "red",
     asp = NA, legacy.axes = TRUE, main = "ROC of KNN")

roc_tree2 <- roc(test_set2$label, prob_tree2, auc = TRUE)
plot(roc_tree2, print.thres = roc_tree2$thresholds[2], col = "red",
     asp = NA, legacy.axes = TRUE, main = "ROC of Tree")

roc_rf2 <- roc(test_set2$label, prob_rf2, auc = TRUE)
index_rf2 <- which.min(abs(roc_rf2$sensitivities - roc_rf2$specificities)[-1])
plot(roc_rf2, print.thres = roc_rf2$thresholds[index_rf2], col = "red",
     asp = NA, legacy.axes = TRUE, main = "ROC of RF")

roc2 <- c(roc_logis2$auc, roc_lda2$auc, roc_qda2$auc, roc_knn2$auc, roc_tree2$auc, roc_rf2$auc)
names(roc2) <- c("Logistics", "LDA", "QDA", "KNN", "Tree", "Random Forest")
roc2

rbind(roc1, roc2)
```



3c.
define other metrics: precision and recall
```{r}
precision <- function(yhat, y) {
    yhat <- factor(yhat)
    y <- factor(y)
    levels(yhat) <- c("-1", "1")
    levels(y) <- c("-1", "1")
    data <- table(yhat, y)
    return(caret::precision(data = data))
}
recall <- function(yhat, y) {
    yhat <- factor(yhat)
    y <- factor(y)
    levels(yhat) <- c("-1", "1")
    levels(y) <- c("-1", "1")
    data <- table(yhat, y)
    return(caret::recall(data = data))
}
```
    
    
3c continued
```{r}
# Split data 1. 
# Train
# Logistic
precision_logis1 <- CVgeneric("logistic", train_val_set1[,c(4:6, 15)], 
                        train_val_set1[, c(3, 15)], lossfn = precision)
precision_logis1
# LDA
precision_lda1 <- CVgeneric("LDA", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = precision)
precision_lda1
# QDA
precision_qda1 <- CVgeneric("QDA", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = precision)
precision_qda1
# KNN
precision_knn1 <- CVgeneric("KNN", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = precision)
precision_knn1
# classification tree
precision_tree1 <- CVgeneric("tree", train_val_set1[,c(4:6, 15)], 
                       train_val_set1[,c(3,15)], lossfn = precision)
precision_tree1
# random forest, take a really long time to run
precision_rf1 <- CVgeneric("randomforest", train_val_set1[,c(4:6, 15)], 
                     train_val_set1[,c(3,15)], lossfn = precision)
precision_rf1
# Summary
precision_split1 <- data.frame(logistic = precision_logis1, 
                                 lda = precision_lda1, qda = precision_qda1, 
                                 KNN = precision_knn1, Tree = precision_tree1, 
                                 rf = precision_rf1)
colnames(precision_split1) <- c("logistic", "LDA", "QDA", "KNN", 
                                  "Tree", "Random Forest")
precision_split1 <- rbind(precision_split1, colMeans(precision_split1))
rownames(precision_split1)[9] <- "precision 1"
precision_split1[9,]
```

3c continued
test precision with cv using data split 2
```{r}
# Split data 2. 
# Train
# Logistic
precision_logis2 <- CVgeneric("logistic", train_val_set2[,c(4:6, 13)], 
                        train_val_set2[, c(3, 13)], lossfn = precision)
precision_logis2
# LDA
precision_lda2 <- CVgeneric("LDA", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = precision)
precision_lda2
# QDA
precision_qda2 <- CVgeneric("QDA", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = precision)
precision_qda2
# KNN
precision_knn2 <- CVgeneric("KNN", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = precision)
precision_knn2
# classification tree
precision_tree2 <- CVgeneric("tree", train_val_set2[,c(4:6, 13)], 
                       train_val_set2[,c(3,13)], lossfn = precision)
precision_tree2
# random forest, take a really long time to run
precision_rf2 <- CVgeneric("randomforest", train_val_set2[,c(4:6, 13)], 
                     train_val_set2[,c(3,13)], lossfn = precision)
precision_rf2
# Summary
precision_split2 <- data.frame(logistic = precision_logis2, 
                                 lda = precision_lda2, qda = precision_qda2, 
                                 KNN = precision_knn2, Tree = precision_tree2, 
                                 rf = precision_rf2)
colnames(precision_split2) <- c("logistic", "LDA", "QDA", "KNN", 
                                  "Tree", "Random Forest")
precision_split2 <- rbind(precision_split2, colMeans(precision_split2))
rownames(precision_split2)[9] <- "precision2"
precision_split2[9,]
```


3c continued
```{r}
# Split data 1. 
# Train
# Logistic
recall_logis1 <- CVgeneric("logistic", train_val_set1[,c(4:6, 15)], 
                        train_val_set1[, c(3, 15)], lossfn = recall)
recall_logis1
# LDA
recall_lda1 <- CVgeneric("LDA", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = recall)
recall_lda1
# QDA
recall_qda1 <- CVgeneric("QDA", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = recall)
recall_qda1
# KNN
recall_knn1 <- CVgeneric("KNN", train_val_set1[,c(4:6, 15)], 
                      train_val_set1[,c(3,15)], lossfn = recall)
recall_knn1
# classification tree
recall_tree1 <- CVgeneric("tree", train_val_set1[,c(4:6, 15)], 
                       train_val_set1[,c(3,15)], lossfn = recall)
recall_tree1
# random forest, take a really long time to run
recall_rf1 <- CVgeneric("randomforest", train_val_set1[,c(4:6, 15)], 
                     train_val_set1[,c(3,15)], lossfn = recall)
recall_rf1
# Summary
recall_split1 <- data.frame(logistic = recall_logis1, 
                                 lda = recall_lda1, qda = recall_qda1, 
                                 KNN = recall_knn1, Tree = recall_tree1, 
                                 rf = recall_rf1)
colnames(recall_split1) <- c("logistic", "LDA", "QDA", "KNN", 
                                  "Tree", "Random Forest")
recall_split1 <- rbind(recall_split1, colMeans(recall_split1))
rownames(recall_split1)[9] <- "recall1"
recall_split1[9,]
```

    
3c continued
test recall on data split 2
```{r}
# Split data 2. 
# Train
# Logistic
recall_logis2 <- CVgeneric("logistic", train_val_set2[,c(4:6, 13)], 
                        train_val_set2[, c(3, 13)], lossfn = recall)
recall_logis2
# LDA
recall_lda2 <- CVgeneric("LDA", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = recall)
recall_lda2
# QDA
recall_qda2 <- CVgeneric("QDA", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = recall)
recall_qda2
# KNN
recall_knn2 <- CVgeneric("KNN", train_val_set2[,c(4:6, 13)], 
                      train_val_set2[,c(3,13)], lossfn = recall)
recall_knn2
# classification tree
recall_tree2 <- CVgeneric("tree", train_val_set2[,c(4:6, 13)], 
                       train_val_set2[,c(3,13)], lossfn = recall)
recall_tree2
# random forest, take a really long time to run
recall_rf2 <- CVgeneric("randomforest", train_val_set2[,c(4:6, 13)], 
                     train_val_set2[,c(3,13)], lossfn = recall)
recall_rf2
# Summary
recall_split2 <- data.frame(logistic = recall_logis2, 
                                 lda = recall_lda2, qda = recall_qda2, 
                                 KNN = recall_knn2, Tree = recall_tree2, 
                                 rf = recall_rf2)
colnames(recall_split2) <- c("logistic", "LDA", "QDA", "KNN", 
                                  "Tree", "Random Forest")
recall_split2 <- rbind(recall_split2, colMeans(recall_split2))
rownames(recall_split2)[9] <- "recall2"
recall_split2[9,]
```
    
3c continued
tabluate the result for precision and recall for 2 data splits
```{r}
precision_recall <- as.data.frame(rbind(precision_split1[9,],
                                        precision_split2[9,], 
                                        recall_split1[9,], 
                                        recall_split2[9,]))
rownames(precision_recall) <- c("precision 1", "precision 2", "recall 1",
                                "recall 2")
precision_recall
```

4a diagnostic plot for random forest model on testset 1 and testset 2
```{r}
plot(randomforest_model1, main = "Random forest model on data split 1")
legend(x = 290, y = 0.18, c("class -1 (not cloudy)", "out-of-bag", 
                "class 1 (cloudy)"), 
       fill = c("red", "black", "green"))
plot(randomforest_model2, main = "Random forest model on data split 2")
legend(x = 290, y = 0.15, c("class -1 (not cloudy)", "out-of-bag", 
                            "class 1 (cloudy)"), 
       fill = c("red", "black", "green"))
```


4b.

misclassification region pattern RF
```{r}
yhat_rf1_i1 <- predict(randomforest_model1, helper11[[4]], type = "response")
yhat_rf1_i2 <- predict(randomforest_model1, helper12[[4]], type = "response")
yhat_rf1_i3 <- predict(randomforest_model1, helper13[[4]], type = "response")

yhat_rf2_i1 <- predict(randomforest_model2, helper1[[4]], type = "response")
yhat_rf2_i2 <- predict(randomforest_model2, helper2[[4]], type = "response")
yhat_rf2_i3 <- predict(randomforest_model2, helper3[[4]], type = "response")


misindex_rf1_i1 <- which(yhat_rf1_i1 != helper11[[4]]$label)
misindex_rf1_i2 <- which(yhat_rf1_i2 != helper12[[4]]$label)
misindex_rf1_i3 <- which(yhat_rf1_i3 != helper13[[4]]$label)

misindex_rf2_i1 <- which(yhat_rf2_i1 != helper1[[4]]$label)
misindex_rf2_i2 <- which(yhat_rf2_i2 != helper2[[4]]$label)
misindex_rf2_i3 <- which(yhat_rf2_i3 != helper3[[4]]$label)


g1 <- ggplot() + 
  geom_raster(aes(x = helper11[[4]]$x, y = helper11[[4]]$y)) +
  geom_point(aes(x = helper11[[4]]$x[misindex_rf1_i1], y = helper11[[4]]$y[misindex_rf1_i1],
                 shape = factor(helper11[[4]]$label[misindex_rf1_i1]),
                 color = factor(helper11[[4]]$label[misindex_rf1_i1])),
             size= 0.8, show.legend = F) +
  scale_color_manual(values = c("#1b9e77", "#d95f02")) +
  theme_bw() + 
  ggtitle("Image1:Misclassification") +
  xlab("x") +
  ylab("y")
g2 <- ggplot() + 
  geom_raster(aes(x = helper12[[4]]$x, y = helper12[[4]]$y)) +
  geom_point(aes(x = helper12[[4]]$x[misindex_rf1_i2], y = helper12[[4]]$y[misindex_rf1_i2],
                 shape = factor(helper12[[4]]$label[misindex_rf1_i2]),
                 color = factor(helper12[[4]]$label[misindex_rf1_i2])),
             size= 0.8, show.legend = F) +
  scale_color_manual(values = c("#1b9e77", "#d95f02")) +
  theme_bw() + 
  ggtitle("Image2:Misclassification") +
  xlab("x") +
  ylab("y")
g3 <- ggplot() + 
  geom_raster(aes(x = helper13[[4]]$x, y = helper13[[4]]$y)) +
  geom_point(aes(x = helper13[[4]]$x[misindex_rf1_i3], y = helper13[[4]]$y[misindex_rf1_i3],
                 shape = factor(helper13[[4]]$label[misindex_rf1_i3]),
                 color = factor(helper13[[4]]$label[misindex_rf1_i3])),
             size= 0.8, show.legend = F) +
  scale_color_manual(values = c("#1b9e77", "#d95f02")) +
  theme_bw() + 
  ggtitle("Image3:Misclassification") +
  xlab("x") +
  ylab("y")

g4 <- ggplot() + 
  geom_raster(aes(x = helper1[[4]]$x, y = helper1[[4]]$y)) +
  geom_point(aes(x = helper1[[4]]$x[misindex_rf2_i1], y = helper1[[4]]$y[misindex_rf2_i1],
                 shape = factor(helper1[[4]]$label[misindex_rf2_i1]),
                 color = factor(helper1[[4]]$label[misindex_rf2_i1])),
             size= 0.8, show.legend = F) +
  scale_color_manual(values = c("#1b9e77", "#d95f02")) +
  theme_bw() + 
  ggtitle("Image1:Misclassification") +
  xlab("x") +
  ylab("y")
g5 <- ggplot() + 
  geom_raster(aes(x = helper2[[4]]$x, y = helper2[[4]]$y)) +
  geom_point(aes(x = helper2[[4]]$x[misindex_rf2_i2], y = helper2[[4]]$y[misindex_rf2_i2],
                 shape = factor(helper2[[4]]$label[misindex_rf2_i2]),
                 color = factor(helper2[[4]]$label[misindex_rf2_i2])),
             size= 0.8, show.legend = F) +
  scale_color_manual(values = c("#1b9e77", "#d95f02")) +
  theme_bw() + 
  ggtitle("Image2:Misclassification") +
  xlab("x") +
  ylab("y")
g6 <- ggplot() + 
  geom_raster(aes(x = helper3[[4]]$x, y = helper3[[4]]$y)) +
  geom_point(aes(x = helper3[[4]]$x[misindex_rf2_i3], y = helper3[[4]]$y[misindex_rf2_i3],
                 shape = factor(helper3[[4]]$label[misindex_rf2_i3]),
                 color = factor(helper3[[4]]$label[misindex_rf2_i3])),
             size= 0.8, show.legend = F) +
  scale_color_manual(values = c("#1b9e77", "#d95f02")) +
  theme_bw() + 
  ggtitle("Image3:Misclassification") +
  xlab("x") +
  ylab("y")
jpeg("misclassification region.jpeg", quality = 100, width = 800, height = 600)
grid.arrange(g1, g2, g3, g4, g5, g6, nrow = 2)
dev.off()

```

misclassification feature pattern RF
data 1
```{r}
gg1_NDAI_mis <- gg1_NDAI + 
  geom_point(aes(x = factor(helper11[[4]]$label[misindex_rf1_i1]), y = helper11[[4]]$NDAI[misindex_rf1_i1]),
             color = "red", shape = 4, alpha = 0.5) 

gg1_SD_mis <- gg1_SD + 
  geom_point(aes(x = factor(helper11[[4]]$label[misindex_rf1_i1]), y = helper11[[4]]$SD[misindex_rf1_i1]),
             color = "red", shape = 4, alpha = 0.5) 

gg1_CORR_mis <- gg1_CORR + 
  geom_point(aes(x = factor(helper11[[4]]$label[misindex_rf1_i1]), y = helper11[[4]]$CORR[misindex_rf1_i1]),
             color = "red", shape = 4, alpha = 0.5) 

gg2_NDAI_mis <- gg2_NDAI + 
  geom_point(aes(x = factor(helper12[[4]]$label[misindex_rf1_i2]), y = helper12[[4]]$NDAI[misindex_rf1_i2]),
             color = "red", shape = 4, alpha = 0.5) 

gg2_SD_mis <- gg2_SD + 
  geom_point(aes(x = factor(helper12[[4]]$label[misindex_rf1_i2]), y = helper12[[4]]$SD[misindex_rf1_i2]),
             color = "red", shape = 4, alpha = 0.5) 

gg2_CORR_mis <- gg2_CORR + 
  geom_point(aes(x = factor(helper12[[4]]$label[misindex_rf1_i2]), y = helper12[[4]]$CORR[misindex_rf1_i2]),
             color = "red", shape = 4, alpha = 0.5) 

gg3_NDAI_mis <- gg3_NDAI + 
  geom_point(aes(x = factor(helper13[[4]]$label[misindex_rf1_i3]), y = helper13[[4]]$NDAI[misindex_rf1_i3]),
             color = "red", shape = 4, alpha = 0.5) 
gg3_SD_mis <- gg3_SD + 
  geom_point(aes(x = factor(helper13[[4]]$label[misindex_rf1_i3]), y = helper13[[4]]$SD[misindex_rf1_i3]),
             color = "red", shape = 4, alpha = 0.5) 
gg3_CORR_mis <- gg3_CORR + 
  geom_point(aes(x = factor(helper13[[4]]$label[misindex_rf1_i3]), y = helper13[[4]]$CORR[misindex_rf1_i3]),
             color = "red", shape = 4, alpha = 0.5) 

jpeg("misclass_feature.jpeg", quality = 100)
grid_mis <- grid.arrange(gg1_NDAI_mis, gg1_SD_mis, gg1_CORR_mis, 
             gg2_NDAI_mis, gg2_SD_mis, gg2_CORR_mis, 
             gg3_NDAI_mis, gg3_SD_mis, gg3_CORR_mis, 
             nrow = 3)
dev.off()

grid.arrange(gg1_NDAI_mis, gg1_SD_mis, gg1_CORR_mis, 
             gg2_NDAI_mis, gg2_SD_mis, gg2_CORR_mis, 
             gg3_NDAI_mis, gg3_SD_mis, gg3_CORR_mis, 
             nrow = 3)
```
data 2
```{r}
gg1_NDAI_mis2 <- gg1_NDAI + 
  geom_point(aes(x = factor(helper1[[4]]$label[misindex_rf2_i1]), y = helper1[[4]]$NDAI[misindex_rf2_i1]),
             color = "blue", shape = 4, alpha = 0.7)

gg1_SD_mis2 <- gg1_SD + 
  geom_point(aes(x = factor(helper1[[4]]$label[misindex_rf2_i1]), y = helper1[[4]]$SD[misindex_rf2_i1]),
             color = "blue", shape = 4, alpha = 0.7)

gg1_CORR_mis2 <- gg1_CORR + 
  geom_point(aes(x = factor(helper1[[4]]$label[misindex_rf2_i1]), y = helper1[[4]]$CORR[misindex_rf2_i1]),
             color = "blue", shape = 4, alpha = 0.7)

gg2_NDAI_mis2 <- gg2_NDAI + 
  geom_point(aes(x = factor(helper2[[4]]$label[misindex_rf2_i2]), y = helper2[[4]]$NDAI[misindex_rf2_i2]),
             color = "blue", shape = 4, alpha = 0.7)

gg2_SD_mis2 <- gg2_SD + 
  geom_point(aes(x = factor(helper2[[4]]$label[misindex_rf2_i2]), y = helper2[[4]]$SD[misindex_rf2_i2]),
             color = "blue", shape = 4, alpha = 0.7)

gg2_CORR_mis2 <- gg2_CORR + 
  geom_point(aes(x = factor(helper2[[4]]$label[misindex_rf2_i2]), y = helper2[[4]]$CORR[misindex_rf2_i2]),
             color = "blue", shape = 4, alpha = 0.7)

gg3_NDAI_mis2 <- gg3_NDAI + 
  geom_point(aes(x = factor(helper3[[4]]$label[misindex_rf2_i3]), y = helper3[[4]]$NDAI[misindex_rf2_i3]),
             color = "blue", shape = 4, alpha = 0.7)

gg3_SD_mis2 <- gg3_SD + 
  geom_point(aes(x = factor(helper3[[4]]$label[misindex_rf2_i3]), y = helper3[[4]]$SD[misindex_rf2_i3]),
             color = "blue", shape = 4, alpha = 0.7)

gg3_CORR_mis2 <- gg3_CORR + 
  geom_point(aes(x = factor(helper3[[4]]$label[misindex_rf2_i3]), y = helper3[[4]]$CORR[misindex_rf2_i3]),
             color = "blue", shape = 4, alpha = 0.7)

jpeg("misclass_feature2.jpeg", quality = 100)
grid_mis2 <- grid.arrange(gg1_NDAI_mis2, gg1_SD_mis2, gg1_CORR_mis2, 
             gg2_NDAI_mis2, gg2_SD_mis2, gg2_CORR_mis2, 
             gg3_NDAI_mis2, gg3_SD_mis2, gg3_CORR_mis2, 
             nrow = 3)
dev.off()

grid.arrange(gg1_NDAI_mis2, gg1_SD_mis2, gg1_CORR_mis2, 
             gg2_NDAI_mis2, gg2_SD_mis2, gg2_CORR_mis2, 
             gg3_NDAI_mis2, gg3_SD_mis2, gg3_CORR_mis2, 
             nrow = 3)
```
    
    
    
      
4c. create a better classifier by adjusting the cutoff value in random forest model.    
```{r}
new_rf1 <- randomForest(as.factor(label) ~ NDAI + CORR + SD, 
                        cutoff = c(0.48, 0.52), 
                        data = train_val_set1)
new_yhat_rf1 <- as.numeric(as.character(predict(new_rf1, test_set1, type = "class")))
new_rf_test_acc1 <- 1 - loss(as.numeric(new_yhat_rf1), test_set1$label)
new_rf_test_acc1
```

4c continued
create a new random forest model 
```{r}
new_rf2 <- randomForest(as.factor(label) ~ NDAI + CORR + SD, 
                        cutoff = c(0.48, 0.52), 
                        data = train_val_set2)
new_yhat_rf2 <- as.numeric(as.character(predict(new_rf2, test_set2, type = "class")))
new_rf_test_acc2 <- 1 - loss(as.numeric(new_yhat_rf2), test_set2$label)
new_rf_test_acc2
```
    
plot thoes model to compare them with previous model with default values.
```{r}
plot(new_rf1, main = "new random forest model on data split 1")
legend(x = 290, y = 0.20, c("class -1 (not cloudy)", "out-of-bag", 
                "class 1 (cloudy)"), 
       fill = c("red", "black", "green"))
plot(new_rf2, main = "new random forest model on data split 2")
legend(x = 290, y = 0.17, c("class -1 (not cloudy)", "out-of-bag", 
                            "class 1 (cloudy)"), 
       fill = c("red", "black", "green"))
```


